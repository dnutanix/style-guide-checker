<ac:layout>
  <ac:layout-section ac:type="single">
    <ac:layout-cell>
      <p>
        <strong>Table of Contents</strong>
      </p>
      <p>
        <ac:structured-macro ac:macro-id="c15e0e6b-8b7f-4df1-aea8-95370f3c06f3" ac:name="toc" ac:schema-version="1"/>
      </p>
      <h1>Phoenix Shell Special Cases</h1>
      
      <p>
        <span style="font-size: 24.0px;letter-spacing: -0.01em;">Overview</span>
      </p>
      <p>This chapter covers two critical recovery scenarios that occasionally occur in production environments and require specialized knowledge to resolve safely. These procedures address complex situations where standard Phoenix operations have been performed incorrectly or where filesystem corruption requires targeted repair without data loss. Both scenarios require careful execution and senior technical oversight to prevent further complications or data loss.</p>
      <ac:structured-macro ac:macro-id="chapter-overview" ac:name="info" ac:schema-version="1">
        <ac:parameter ac:name="title">What You Will Learn</ac:parameter>
        <ac:rich-text-body>
          <p>By the end of this chapter, you will be able to:</p>
          <ul>
            <li>
              <strong>Recover from improper CVM installations</strong> - Restore cluster recognition of CVMs that were incorrectly reimaged using "Install CVM" on active cluster nodes</li>
            <li>
              <strong>Execute cluster reintegration procedures</strong> - Use the boot_disk_replace script to restore configuration and manage disk ID conflicts</li>
            <li>
              <strong>Repair root partition corruption</strong> - Fix EXT4 filesystem corruption in CVM root partitions without formatting /home or losing configuration data</li>
            <li>
              <strong>Apply targeted filesystem recovery</strong> - Use AOS upgrade scripts to format and flip root partitions while preserving user data</li>
            <li>
              <strong>Follow safety protocols</strong> - Understand critical warnings and approval requirements for high-risk recovery procedures</li>
            <li>
              <strong>Validate recovery success</strong> - Perform post-recovery verification to ensure system stability and data integrity</li>
          </ul>
        </ac:rich-text-body>
      </ac:structured-macro>
    </ac:layout-cell>
  </ac:layout-section>
  <ac:layout-section ac:type="single">
    <ac:layout-cell>
      <h1>Special Case 1: Recovery from Improper CVM Installation</h1>
      <p>This section addresses the critical situation where "Install CVM" was incorrectly run on a node that remained part of an active cluster, requiring specialized recovery procedures to restore cluster functionality.</p>
      
      <ac:structured-macro ac:name="expand" ac:schema-version="1">
        <ac:parameter ac:name="title">Complete CVM Installation Recovery Guide</ac:parameter>
        <ac:rich-text-body>
        
      <ac:structured-macro ac:name="warning" ac:schema-version="1">
        <ac:parameter ac:name="title">Critical Safety Information</ac:parameter>
        <ac:rich-text-body>
          <p><strong>Please read ALL warnings below before proceeding with this procedure.</strong> This is a high-risk recovery operation that requires strict adherence to safety protocols.</p>
        </ac:rich-text-body>
      </ac:structured-macro>
      <ac:structured-macro ac:macro-id="ffb1e1b8-9c03-413e-8a85-196f605511c9" ac:name="warning" ac:schema-version="1">
        <ac:rich-text-body>
          <p>Never intentionally run "Install CVM" using Phoenix on a node that is still part of a running cluster as it is destructive to user data and may result in data loss if there was an issue restoring Resiliency. If a complete reinstall of AOS is <em>absolutely necessary</em>, the correct course of action is to first logically remove the node from the cluster (Prism Hardware Page &gt; Diagram &gt; Remove Host or CLI method <a href="https://nutanix.my.salesforce.com/articles/Knowledge_Base/Removing-Unreachable-Or-Powered-Off-Node-From-Nutanix-Cluster">KB-2379</a>) and let this process complete to 100%. Only once node removal is completely done should Phoenix "Install CVM" be performed. After cleanly reimaging the node, the Cluster Expand feature can be used to add it back into the original.</p>
          <p>For 3-Node clusters where node removal is not an option, Repair CVM should be the only option considered. Make sure to observe ALL of the warnings documented at the start of <a href="https://confluence.eng.nutanix.com:8443/display/STK/SVM+Rescue%3A+Re-Creating+a+CVM">
              <strong>this page</strong>
            </a> before proceeding with these actions.</p>
        </ac:rich-text-body>
      </ac:structured-macro>
      <ac:structured-macro ac:macro-id="214a01c4-0f64-4670-953a-5420fe022d91" ac:name="warning" ac:schema-version="1">
        <ac:rich-text-body>
          <p>Make sure that any Phoenix ISOs that contain an AOS binary are deleted from a customer's cluster/workstation before disengaging.</p>
        </ac:rich-text-body>
      </ac:structured-macro>
      <ac:structured-macro ac:macro-id="4c0db8b1-e745-4e76-9526-d15b11d83b7a" ac:name="warning" ac:schema-version="1">
        <ac:rich-text-body>
          <p>Please educate any customer/partner/SRE who mistakenly runs "Install CVM" on a clustered node that this is an unsupported workflow and can lead to data loss. Phoenix containing AOS is only to be used under the direct supervision of Nutanix Support.</p>
        </ac:rich-text-body>
      </ac:structured-macro>
      <p>
        <br/>
      </p>
      <p>The "Install CVM" option in Phoenix formats all user data and cluster configuration information from all disks associated with the node. At the end of this process, the CVM should be in a discoverable state where it can be added to a cluster using the Expand Cluster feature. This Phoenix option is reserved exclusively for imaging/reimaging of a brand new node or one that has already been logically removed from its original cluster.</p>
      <p>This Phoenix option is distinctly different from "Repair CVM", which only formats the CVM infrastructure partitions and leaves the customer data partitions intact. You can read about all the functions available in Phoenix <strong>
          <a href="https://confluence.eng.nutanix.com:8443/display/STK/Phoenix%3A+Running+the+Installer#Phoenix:RunningtheInstaller-Phase3:TheNutanixInstallerMenu-ChoosingtheAction">here</a>
        </strong>. If you have any doubts whatsoever about a particular course of action, stop and consult an STL or a member of DevEx.</p>
      <p>
        <br/>
      </p>
      <h2>Process of Recovery</h2>
      <ol>
        <li>Confirm that node which was reimaged is still part of the original cluster by running the "cluster status" command from a separate CVM. You should see the affected CVM listed in a "Down" state.</li>
        <li>Confirm that the reimaged node is in an unconfigured state by running "cluster status". You should see output indicating, "Cluster is currently unconfigured. Please create the cluster."</li>
        <li>Educate the customer and/or SRE that this was an unsupported action and should never be performed on a clustered node due to the risk of data loss.</li>
        <li>Make sure that there is no active ring change going on in Cassandra. Consult an STL or a member of DevEx before proceeding if you see an active ring change.<br/>
          <ac:structured-macro ac:macro-id="d06caa54-7e6f-403a-b105-229182640b08" ac:name="noformat" ac:schema-version="1">
            <ac:plain-text-body><![CDATA[nutanix@CVM:~$ progress_monitor_cli -fetchall
nutanix@CVM:~$ panacea_cli show_zeus_config | grep dyn_ -A16]]></ac:plain-text-body>
          </ac:structured-macro>
        </li>
        <li>From a separate CVM in the same cluster, execute the boot_disk_replace script as documented <strong>
            <a href="https://confluence.eng.nutanix.com:8443/display/STK/SVM+Rescue%3A+Re-Creating+a+CVM#SVMRescue:Re-CreatingaCVM-Post-Rescue:Runningtheboot_disk_replaceScript">here</a>
          </strong>, from Step #5 onwards. <u>You do <em>not</em> need to touch the CVM boot ISO.</u> This script will restore original configuration files onto the unconfigured CVM so that it can be once-again recognized by the running cluster.</li>
        <li>Disk removal tasks should automatically appear to logically remove the original disk IDs for the drives associated with the original generation of the node (before the disks were reformatted). Monitor these tasks in Prism and using <strong>
            <a href="http://portal.nutanix.com/kb/16236">KB-16236</a>
          </strong> to ensure that they are able to complete to 100%. <br/>
          <br/>In the unlikely case that you do not see any disk removal tasks get initiated, proceed with the following steps to manually mark the original Disk IDs for removal:<ol>
            <li>
              <p class="auto-cursor-target">Using a working CVM in the same cluster, obtain a list of the original disk_ids from the output of "panacea_cli show_disk_list" or directly from zeus_config_printer. You will find that the original disks will have a lower-value ID number than do the new IDs of the same drives after Phoenix was run. This is because Foundation issues Disk IDs from the beginning of the number range (usually 2-3 digits long) while Phoenix numbers drives from the opposite end of the number range (7+ digits long). Make sure only to remove the original disk IDs, leaving the new ones in-place if they appear.<br/>
                <br/>Replace the service_vm_id number in the command below with the logical ID of the reimaged CVM to get the list of all (original and new) Disk IDs associated with this node.</p>
              <ac:structured-macro ac:macro-id="9c72b720-110c-4ec5-9211-d4769afbb70d" ac:name="noformat" ac:schema-version="1">
                <ac:plain-text-body><![CDATA[nutanix@CVM:~$ panacea_cli show_disk_list

==> DISK_LIST  (count: 24)
----------------------------------------------------------------------------------------------------------------------------------
  DISK_ID     SVM            SLOT   DEVICE   SERIAL           TIER       SIZE               CURATOR_RSV        ADDITIONAL_INFO
----------------------------------------------------------------------------------------------------------------------------------
  57          xx.yy.zz.16    1      None     S456NY0M204083   SSD-SATA   3130.52193451 GB   0                  contains_metadata
  58          xx.yy.zz.16    2      None     S456NY0M203828   SSD-SATA   3130.58200319 GB   0                  contains_metadata
  53          xx.yy.zz.16    3      None     W461SQQK         DAS-SATA   1701.49361813 GB   36.5912606046 GB
  54          xx.yy.zz.16    4      None     W461HJAB         DAS-SATA   1678.08487873 GB   60.0 GB
  55          xx.yy.zz.16    5      None     W461RYCD         DAS-SATA   1700.67986813 GB   36.5737606045 GB
  59          xx.yy.zz.16    6      None     W461PW78         DAS-SATA   1700.67986813 GB   36.5737606045 GB
...]]></ac:plain-text-body>
              </ac:structured-macro>
              <p class="auto-cursor-target">
                <br/>
              </p>
            </li>
            <li>
              <p class="auto-cursor-target">Issue logical removal tasks for the <em>original</em> disk IDs, allowing each to complete before issuing the next removal.</p>
              <ac:structured-macro ac:macro-id="dae253af-a5da-4031-9ad9-f60b92a574d6" ac:name="noformat" ac:schema-version="1">
                <ac:plain-text-body><![CDATA[nutanix@CVM:10.xx.xx.121:~$ ncli disk rm-start id=<disk_id> force=true]]></ac:plain-text-body>
              </ac:structured-macro>
              <p class="auto-cursor-target">
                <br/>
              </p>
            </li>
            <li>
              <p class="auto-cursor-target">Once all the disks have been successfully removed and no longer appear in the disk_list section of zeus_config_printer, restart the Hades service on the reimaged CVM.</p>
              <ac:structured-macro ac:macro-id="39a096cb-30c5-4e8a-bf3e-e906a5e92a77" ac:name="noformat" ac:schema-version="1">
                <ac:plain-text-body><![CDATA[nutanix@CVM:10.xx.xx.122:~$ sudo /usr/local/nutanix/cluster/bin/hades restart]]></ac:plain-text-body>
              </ac:structured-macro>
              <p class="auto-cursor-target">
                <br/>
              </p>
            </li>
            <li>
              <p class="auto-cursor-target">Wait 2 minutes for Hades to stabilize, then restart the Genesis service on the reimaged CVM.</p>
              <ac:structured-macro ac:macro-id="54f3892a-6afb-4f50-8f38-17f0d659b875" ac:name="noformat" ac:schema-version="1">
                <ac:plain-text-body><![CDATA[nutanix@CVM:10.xx.xx.122:~$ genesis restart]]></ac:plain-text-body>
              </ac:structured-macro>
              <p class="auto-cursor-target">
                <br/>
              </p>
            </li>
            <li>Wait 4 minutes for Genesis to stabilize, then go to the Prism Hardware Diagram and check if the drives have been accepted under their new logical IDs or if they appear in Red as shown in the example below.<br/>
              <ac:image ac:height="156">
                <ri:attachment ri:filename="image2023-7-10_13-27-19.png"/>
              </ac:image>
            </li>
            <li>If the drives are in a normal color, then skip this step. If the drives appear in Red, navigate to the Prism Hardware diagram. Select each drive one at a time, starting with the Metadata drives, and select 'Repartition and Add' until all the disks are added back.</li>
            <li>
              <p class="auto-cursor-target">Watch the CVMs services to see if Stargate and other CDP services are able to start and that everything remains stable.</p>
              <ac:structured-macro ac:macro-id="52b28f5d-ae18-47fc-a37d-9e7b4fc343b1" ac:name="noformat" ac:schema-version="1">
                <ac:plain-text-body><![CDATA[nutanix@CVM:10.xx.xx.122:~$ watch -d genesis status]]></ac:plain-text-body>
              </ac:structured-macro>
              <p class="auto-cursor-target">
                <br/>
              </p>
            </li>
            <li>If any services do not start-up, consult the logs below for signs of issues. You can also find a reference to the pertinent genesis log signatures <a href="https://confluence.eng.nutanix.com:8443/display/STK/Genesis+Troubleshooting">
                <strong>here</strong>
              </a>.<ol>
                <li>genesis.out</li>
                <li>hades.out</li>
                <li>cassandra_monitor.INFO</li>
              </ol>
            </li>
          </ol>
        </li>
      </ol>
        </ac:rich-text-body>
      </ac:structured-macro>
      
      <h1>Special Case 2: Root Partition Corruption Repair</h1>
      <p>This section covers a specialized procedure for repairing EXT4 filesystem corruption in CVM root partitions without requiring a full svmrescue operation or formatting the /home partition, preserving CVM configuration and user data.</p>
      
      <ac:structured-macro ac:name="expand" ac:schema-version="1">
        <ac:parameter ac:name="title">Complete Root Partition Recovery Guide</ac:parameter>
        <ac:rich-text-body>
      <ac:structured-macro ac:macro-id="55fdb562-abd4-4e93-b166-b29bd009eb62" ac:name="warning" ac:schema-version="1">
        <ac:parameter ac:name="title">PLEASE READ</ac:parameter>
        <ac:rich-text-body>
          <ul>
            <li>This procedure is ONLY to be followed upon receiving approval from an STL or a member of the DevEx Team. Approval must be documented using the Cluster Modification Template in the internal Case Comments.</li>
            <li>
              <span style="color: rgb(51,51,51);">This procedure is only for CVM and should not be followed for PCVM.</span>
            </li>
          </ul>
        </ac:rich-text-body>
      </ac:structured-macro>
      <p>
        <br/>
      </p>
      <p>In <a href="http://portal.nutanix.com/kb/13931">KB-13931</a>, which details the troubleshooting process for EXT4 filesystem errors on a CVM, <strong>Action Plan 6</strong> indicates one case where corruption on a non-stargate partition will requires an svmrescue or Phoenix Repair of the CVM in order to recover. The process outlined here offers an alternative to this process when the partition having EXT4 corruption is one of the two 10GB root partitions for AOS <span style="color: rgb(23,43,77);">(e.g., where '/' is mounted)</span>. When the corruption is in the root partition, we can leverage the same scripts used by AOS upgrades to format and flip the active root partition while staying on the same AOS version. The advantage of this method over svmrescue is that we avoid formatting /home, which is destructive of the CVM's configuration and is best avoided if possible. <span style="color: rgb(23,43,77);">This also means you can use this process to fix root partition corruption on Single Node Clusters, where svmrescue is not supported.</span>
      </p>
      <p>EXT4 filesystem errors can occur due to any of the reasons cited below.</p>
      <ul>
        <li>Unclean CVM or host shutdown</li>
        <li>A hardware failure (e.g., CVM boot SSD/NVMe, HBA, chassis/node)</li>
        <li>A software bug (e.g., <a class="external-link" href="https://jira.nutanix.com/browse/ENG-215127" style="text-align: left;">ENG-215127</a>,<span style="color: rgb(23,43,77);">
            <span> </span>
          </span>
          <a class="external-link" href="http://download.nutanix.com/alerts/Field%20Advisory%20%2367v3.pdf" style="text-align: left;">FA#67</a>
          <span style="color: rgb(23,43,77);">,<span> </span>
          </span>
          <a class="external-link" href="http://portal.nutanix.com/kb/7669" style="text-align: left;">KB-7669</a>
          <span style="color: rgb(23,43,77);">)</span>
        </li>
      </ul>
      <ac:structured-macro ac:macro-id="5d16971a-e328-4b86-8bf5-4658bff9d3b2" ac:name="note" ac:schema-version="1">
        <ac:rich-text-body>
          <p>
            <span style="color: rgb(51,51,51);">
              <span> I</span>f filesystem issues are encountered during an upgrade, please consult </span>
            <a class="external-link" href="http://portal.nutanix.com/kb/7669">KB-7669</a>
            <span style="color: rgb(51,51,51);"> to see if that procedure should be followed instead of the one found here. This will also repair the root partition.</span>
          </p>
        </ac:rich-text-body>
      </ac:structured-macro>
      <p>
        <br/>
      </p>
      <ac:structured-macro ac:macro-id="f0e8c0c0-59a4-4efb-839a-8e0a4aa06548" ac:name="note" ac:schema-version="1">
        <ac:rich-text-body>
          <p>Before taking corrective action, it is critical that you follow Action Plan 7 in <a href="http://portal.nutanix.com/kb/13931">KB-13931</a> in an attempt to find the specific root cause for the corruption seen on the customer's cluster. Please refer to <a href="https://confluence.eng.nutanix.com:8443/display/STK/WF%3A+Disk+Debugging">WF: Disk Debugging</a> and the <a href="https://confluence.eng.nutanix.com:8443/display/STK/Using+the+cvm_diagnose+Script#Usingthecvm_diagnoseScript-TroubleshootingWorkflows">cvm_diagnose guide</a> for examples of commands/logs for detecting potential hardware issues.</p>
        </ac:rich-text-body>
      </ac:structured-macro>
      <h2>Additional Use Cases</h2>
      <p>This process is also suitable for use in the following cases:</p>
      <ul>
        <li>There was an improper <span style="color: rgb(23,43,77);">deletion of files in the ‘ / ‘ partition which cannot be manually restored since the files that were deleted were unknown.</span>
        </li>
      </ul>
      <h2>Identifying EXT4 corruption in root partition</h2>
      <p>Please refer to <a href="http://portal.nutanix.com/kb/13931">KB-13931</a> for the full list of steps for locating the source of the filesystem corruption.</p>
      <h3>1. NCC fs_inconsistency_check (KB-8514)</h3>
      <ac:structured-macro ac:macro-id="d6def2a9-2def-457c-af99-10795f22ec1e" ac:name="noformat" ac:schema-version="1">
        <ac:plain-text-body><![CDATA[Node ww.xx.yy.zz:
WARN: 26 EXT4-fs error messages are detected in dmesg. Errors occurred are:
Tue Jun 16 19:41:15 2020: EXT4-fs error: 9 callbacks suppressed
Tue Jun 16 19:41:15 2020: EXT4-fs error (device md0): ext4_lookup:1441: inode #132171: comm snmpd: deleted inode referenced: 131630
Tue Jun 16 19:41:15 2020: EXT4-fs error (device md0): ext4_lookup:1441: inode #132171: comm snmpd: deleted inode referenced: 131630
EXT4-fs errors are detected in tune2fs. Most recent errors are:
disk md0: last error time: wed jun 17 03:49:12 2020]]></ac:plain-text-body>
      </ac:structured-macro>
      <h3>2. Partition or RAID group having corruption is where root is mounted /</h3>
      <ac:structured-macro ac:macro-id="12752b9c-7ff3-413b-b9f2-5c8cfebc4c25" ac:name="noformat" ac:schema-version="1">
        <ac:plain-text-body><![CDATA[nutanix@NTNX-CVM:ww.xx.yy.zz:~$ df -h /root
Filesystem      Size  Used Avail Use% Mounted on
/dev/md0        9.8G  7.1G  2.2G  77% /]]></ac:plain-text-body>
      </ac:structured-macro>
      <p>
        <br/>
      </p>
      <p>
        <span style="font-size: 16.0px;font-weight: bold;letter-spacing: -0.006em;">Requirements</span>
      </p>
      <p>
        <span>In order to use this procedure, both of the following conditions must be satisfied:</span>
      </p>
      <ol>
        <li>
          <span>The Genesis service on the affected CVM must be running and in stable condition. Other services are not required.</span>
        </li>
        <li>
          <span>The <strong>python</strong> and <strong>tools</strong> libraries located in the root partition must not be corrupt.</span>
        </li>
      </ol>
      <p>
        <br/>
      </p>
      <h2>Steps for Resolution</h2>
      <p>
        <span>To recover the CVM run the following steps:</span>
      </p>
      <p>
        <span>1. Run <strong>install.sh</strong> from the current version of the AOS directory and log output to install.out </span>
      </p>
      <ac:structured-macro ac:macro-id="c72f2a0f-311c-4585-800e-0426b9321163" ac:name="code" ac:schema-version="1">
        <ac:parameter ac:name="language">bash</ac:parameter>
        <ac:plain-text-body><![CDATA[nutanix@CVM:~$ bash ~/data/installer/el*/install.sh |& tee >(~/bin/logpipe -o ~/data/logs/install.out) ]]></ac:plain-text-body>
      </ac:structured-macro>
      <p class="auto-cursor-target">
        <br/>
      </p>
      <ac:structured-macro ac:macro-id="24afca72-f955-429f-970d-b82f60f38e0e" ac:name="code" ac:schema-version="1">
        <ac:parameter ac:name="language">erl</ac:parameter>
        <ac:parameter ac:name="title">Example Output</ac:parameter>
        <ac:plain-text-body><![CDATA[Copied svmboot.iso to /
Upgrading SVM and Nutanix packages
/home/nutanix/data/installer/el8.5-release-ganges-7.3-stable-403c2537de51674979c3b35300abff1a7566ac8a/bin/svm_upgrade:168: RuntimeWarning: Trying to access flag cloud_init_network_conf_file before flags were parsed. This will raise an exception in the future.
...
2025-07-14 01:52:31,239Z INFO MainThread svm_upgrade:699 Wrote marker file, contents:

KERNEL=/boot/vmlinuz-5.10.234-2.el8.nutanix.20250429.100236.x86_64
CMDLINE='ro rd_NO_LUKS rd_NO_LVM rd_NO_DM LANG=en_US.UTF-8 SYSFONT=latarcyrheb-sun16 rhgb KEYBOARDTYPE=pc KEYTABLE=us audit=1 audit_backlog_limit=8192 watchdog_thresh=20 nousb nomodeset biosdevname=0 net.ifnames=0 scsi_mod.use_blk_mq=y clocksource=tsc hv_netvsc.ring_size=512 mds=off panic=30 mpt2sas.prot_mask=1 mpt3sas.prot_mask=1 mpt3sas.hbas_to_enumerate=0 mpt2sas.issue_scsi_cmd_to_bringup_drive=0 mpt3sas.issue_scsi_cmd_to_bringup_drive=0 iavmd.direct_assign=1 vmd.direct_assign=1 page_poison=0 slub_debug=n pti=off fips=1 vsyscall=none mitigations=off,eibrs crashkernel=256M rd_MD_UUID=9fccbdd4:a9c805a1:525f081b:1bc43921 root=UUID=30dfa9ca-a9aa-4c4d-aa9a-21fe6e191bd3'
INITRD=/boot/initramfs-5.10.234-2.el8.nutanix.20250429.100236.x86_64.img

2025-07-14 01:52:32,498Z INFO MainThread svm_upgrade:718 Injecting /etc/mdadm.conf into initrd /tmp/bootpart/boot/initramfs-5.10.234-2.el8.nutanix.20250429.100236.x86_64.img
2025-07-14 01:52:37,901Z WARNING MainThread luks_helper.py:74 Unable to check LUKS status on the cluster.
...
2025-07-14 01:52:37,906Z INFO MainThread svm_upgrade:2871 Successfully upgraded the SVM gold image, please reboot update active partition marker on the new boot disk
...
2025-07-14 01:52:40,231Z INFO MainThread svm_upgrade:2892 Running setfiles.sh on the chroot: /tmp/bootpart
2025-07-14 01:53:01,985Z INFO MainThread svm_upgrade:2909 Finished setting /tmp/bootpart up for upgrade
logpipe: read: Broken pipe]]></ac:plain-text-body>
      </ac:structured-macro>
      <p>
        <br/>
      </p>
      <p>
        <span>2. SSH to a working CVM and assign the shutdown token to the affected CVM.</span>
        <span>
          <br/>
        </span>
      </p>
      <p>
        <span>
          <strong>NOTE:</strong> Token passing will only work if all services are up and the cluster is resilient (for more details see <a href="https://confluence.eng.nutanix.com:8443/display/STK/WF%3A+Shutdown+Token">WF: Shutdown Token</a>). If your CVM is partially down, the token pass will fail. In such cases, you may skip this step and move to Step 3. Just make sure there are no problems with any other CVMs in the cluster. If you are unsure, check with an STL or a member of DevEx before proceeding.</span>
      </p>
      <ac:structured-macro ac:macro-id="ff9bb26f-7338-4c57-a9f9-f47a21419e98" ac:name="code" ac:schema-version="1">
        <ac:parameter ac:name="language">erl</ac:parameter>
        <ac:plain-text-body><![CDATA[nutanix@CVM:~$ cluster --shutdown_token_ip <IP_address_of_the_unhealthy_CVM> pass_shutdown_token 
]]></ac:plain-text-body>
      </ac:structured-macro>
      <p>
        <br/>
      </p>
      <ac:structured-macro ac:macro-id="c0947a04-2f48-44e1-874f-82a59187a93b" ac:name="code" ac:schema-version="1">
        <ac:parameter ac:name="language">erl</ac:parameter>
        <ac:parameter ac:name="title">Example Output</ac:parameter>
        <ac:plain-text-body><![CDATA[2025-07-14 01:53:29,115Z INFO MainThread zookeeper_session.py:272 cluster is attempting to connect to Zookeeper (unestablished session (object 0x7f21d6659d90)), host port list zk1:9876,zk2:9876,zk3:9876
2025-07-14 01:53:29,116Z INFO MainThread patterns.py:63 Creating a new instance for ZookeeperSession[('client_id', None), ('connection_timeout', None), ('host_port_list', 'zk1:9876,zk2:9876,zk3:9876'), ('use_zk_mt', None)]
2025-07-14 01:53:29,117Z INFO Dummy-1 zookeeper_session.py:933 ZK session establishment complete, session 0x19804f0502f04dc (object 0x7f21d6659d90), negotiated timeout=20 secs
2025-07-14 01:53:29,127Z INFO MainThread cluster:3607 Executing action pass_shutdown_token on SVMs xx.yy.zz.46,xx.yy.zz.47,xx.yy.zz.48
2025-07-14 01:53:29,134Z INFO MainThread cluster:2999 Passing shutdown token to node with ip xx.yy.zz.46 for nos_upgrade
2025-07-14 01:53:29,259Z INFO MainThread cluster:3771 Success!]]></ac:plain-text-body>
      </ac:structured-macro>
      <p>
        <br/>
      </p>
      <p>
        <span>3. Run </span>
        <strong>finish.sh</strong>
        <span> from the current version of AOS directory and log output to finish.out</span>
      </p>
      <ac:structured-macro ac:macro-id="d9183300-27d0-41dd-9cc5-dc55a29a304d" ac:name="note" ac:schema-version="1">
        <ac:parameter ac:name="title">NOTE</ac:parameter>
        <ac:rich-text-body>
          <p>
            <span>This will reboot CVM immediately (without any guardrails) upon completion. After the CVM boots-up, the boot partition should have flipped and there should be no more signs of corruption in root '/'.</span>
          </p>
        </ac:rich-text-body>
      </ac:structured-macro>
      <p>
        <br/>
      </p>
      <ac:structured-macro ac:macro-id="fe900907-150c-482d-81a0-c5ae49165c0b" ac:name="code" ac:schema-version="1">
        <ac:parameter ac:name="language">bash</ac:parameter>
        <ac:plain-text-body><![CDATA[nutanix@CVM:~$ bash ~/data/installer/el*/finish.sh |&tee >(~/bin/logpipe -o ~/data/logs/finish.out)]]></ac:plain-text-body>
      </ac:structured-macro>
      <p>
        <br/>
      </p>
      <ac:structured-macro ac:macro-id="37a7c27c-4484-4ce6-9252-a0c8653595c6" ac:name="code" ac:schema-version="1">
        <ac:parameter ac:name="language">erl</ac:parameter>
        <ac:parameter ac:name="title">Example Output</ac:parameter>
        <ac:plain-text-body><![CDATA[2025-07-14 01:52:10,654Z INFO finish:661 Stopping all services before reboot.
2025-07-14 01:53:11,991Z INFO cluster_upgrade.py:834 Stopping all services before reboot
2025-07-14 01:54:43,587Z INFO cluster_upgrade.py:842 Stopping services forcefully
2025-07-14 01:55:24,817Z INFO MainThread cluster_upgrade.py:948 Stopping all services before reboot took 29.714791536331177 seconds
2025-07-14 01:55:24,818Z INFO MainThread finish:1203 SVM is running on KVM, need to update the SVM xml config file as part of the upgrade
2025-07-14 01:56:15,122Z INFO cluster_upgrade.py:864 Waiting for local SVM to restart...]]></ac:plain-text-body>
      </ac:structured-macro>
      <p>
        <br/>
      </p>
      <p>
        <span>4. Make sure that after running these steps all the services are UP and running and the CVM environment is properly sourced (Check if it opens as 'nutanix' and not as 'bash' profile).</span>
      </p>
      <p>
        <span>5. Run <strong>NCC</strong> (which contains the fs_inconsistency_check) to make sure there are no signs of any problems with the CVM or the cluster.</span>
      </p>
        </ac:rich-text-body>
      </ac:structured-macro>
    </ac:layout-cell>
  </ac:layout-section>
  <ac:layout-section ac:type="single">
    <ac:layout-cell>
      <ac:structured-macro ac:macro-id="b2c74dff-6cf8-4290-afc7-bb4e233c4efc" ac:name="macrosuite-button" ac:schema-version="1">
        <ac:parameter ac:name="macroId">GCv5oaSEEAVFOvhJcvbde</ac:parameter>
        <ac:parameter ac:name="data">JTdCJTIyYnV0dG9uVHlwZSUyMiUzQSUyMmljb25fcmlnaHQlMjIlMkMlMjJidXR0b25TaXplJTIyJTNBJTIybGFyZ2UlMjIlMkMlMjJidXR0b25UZXh0JTIyJTNBJTIyUHJvY2VlZCUyMHRvJTIwTmV4dCUyMFNlY3Rpb24lMjIlMkMlMjJidXR0b25SYWRpdXMlMjIlM0ExJTJDJTIyYnV0dG9uV2lkdGglMjIlM0EyMCUyQyUyMmJ1dHRvbkNvbG9yJTIyJTNBJTIyJTIzMDAwMDAwMDAlMjIlMkMlMjJidXR0b25Gb250Q29sb3IlMjIlM0ElMjIlMjMwMDY1ZmZmZiUyMiUyQyUyMmJ1dHRvbkJvcmRlckNvbG9yJTIyJTNBJTIyJTIzMDAwMDAwMDAlMjIlMkMlMjJidXR0b25MaW5rJTIyJTNBJTIyJTVDJTIyJTVDJTIyJTIyJTJDJTIyYnV0dG9uTmV3VGFiJTIyJTNBJTIyZmFsc2UlMjIlMkMlMjJidXR0b25OZXdMaW5rJTIyJTNBJTIyJTIyJTJDJTIyYnV0dG9uSWNvbiUyMiUzQSUyMmJvb3RzdHJhcCUyRkFycm93UmlnaHRDaXJjbGUlMjIlMkMlMjJidXR0b25Ib3ZlckNvbG9yJTIyJTNBJTIydHJhbnNwYXJlbnQlMjIlMkMlMjJidXR0b25Cb3JkZXJIb3ZlckNvbG9yJTIyJTNBJTIyJTIzMDAwMDAwMDAlMjIlMkMlMjJidXR0b25Gb250SG92ZXJDb2xvciUyMiUzQSUyMiUyMzAwMDAwMDAwJTIyJTJDJTIyYnV0dG9uSWNvbkhvdmVyQ29sb3IlMjIlM0ElMjIlMjMwMDAwMDAwMCUyMiUyQyUyMmlzQnV0dG9uU2hhZG93T24lMjIlM0F0cnVlJTJDJTIyYnV0dG9uSWNvbkNvbG9yJTIyJTNBJTIyJTIzMDA2NWZmZmYlMjIlMkMlMjJidXR0b25XaWR0aERldGVjdGlvbiUyMiUzQTYwJTJDJTIyZW1vamlFbmFibGVkJTIyJTNBZmFsc2UlMkMlMjJlbW9qaSUyMiUzQSUyMiU3QiU3RCUyMiU3RA==</ac:parameter>
      </ac:structured-macro>
    </ac:layout-cell>
  </ac:layout-section>
</ac:layout>
